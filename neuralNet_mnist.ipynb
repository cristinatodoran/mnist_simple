{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "import tensorflow as tf\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNet:\n",
    "    \n",
    "    def __init__(self,nClasses,image_size,learningRate):\n",
    "        \n",
    "        self.n_classes= nClasses\n",
    "        self.image_size = image_size\n",
    "        self.learningRate = learningRate\n",
    "        self.X = tf.placeholder(dtype='float', shape=[None, self.image_size], name='X') # height, width\n",
    "        self.Y = tf.placeholder(dtype='int32', name='Y')\n",
    "    \n",
    "    def model(self,num_nodehl1,num_nodehl2,num_nodehl3):\n",
    "        \n",
    "        self.n_nodes_hl1 = num_nodehl1\n",
    "        self.n_nodes_hl2 = num_nodehl2\n",
    "        self.n_nodes_hl3 = num_nodehl3\n",
    "        \n",
    "        \n",
    "        self.hidden_layer_1 = {'weights': tf.Variable(tf.random_normal([self.image_size, self.n_nodes_hl1])), \n",
    "                         'biases' : tf.Variable(tf.random_normal([self.n_nodes_hl1]))}\n",
    "\n",
    "        self.hidden_layer_2 = {'weights': tf.Variable(tf.random_normal([self.n_nodes_hl1, self.n_nodes_hl2])), \n",
    "                         'biases' : tf.Variable(tf.random_normal([self.n_nodes_hl2]))}\n",
    "\n",
    "        self.hidden_layer_3 = {'weights': tf.Variable(tf.random_normal([self.n_nodes_hl2, self.n_nodes_hl3])), \n",
    "                         'biases' : tf.Variable(tf.random_normal([self.n_nodes_hl3]))}\n",
    "\n",
    "        self.output_layer = {'weights': tf.Variable(tf.random_normal([self.n_nodes_hl3, self.n_classes])), \n",
    "                         'biases' : tf.Variable(tf.random_normal([self.n_classes]))}\n",
    "        \n",
    "         # X * weights + biases\n",
    "        self.l1 = tf.add(tf.matmul(self.X, self.hidden_layer_1['weights']), self.hidden_layer_1['biases'])\n",
    "        self.l1 = tf.nn.relu(self.l1)\n",
    "\n",
    "        self.l2 = tf.add(tf.matmul(self.l1, self.hidden_layer_2['weights']), self.hidden_layer_2['biases'])\n",
    "        self.l2 = tf.nn.relu(self.l2)\n",
    "\n",
    "        self.l3 = tf.add(tf.matmul(self.l2, self.hidden_layer_3['weights']), self.hidden_layer_3['biases'])\n",
    "        self.l3 = tf.nn.relu(self.l3)\n",
    "\n",
    "        self.predicted_class = tf.add(tf.matmul(self.l3, self.output_layer['weights']), self.output_layer['biases'])\n",
    "\n",
    "        return self.predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(train_dir=\"data/mnist\", one_hot = False)\n",
    "x_batch, y_true_batch = mnist.train.next_batch(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 28 * 28 # input images are 28 x 28\n",
    "n_classes = 10 # numbers 0 - 9 in the dataset\n",
    "learningRate = 1e-3\n",
    "num_nodehl1 = 700\n",
    "num_nodehl2 =700\n",
    "num_nodehl3 =700\n",
    "train_batch_size = 128 # manipulate weights by 128 features at a time\n",
    "epochs = 50\n",
    "def train():\n",
    "    \n",
    "    neuralNet_ = neuralNet(n_classes,image_size,learningRate)\n",
    "    print (\"Neural Net initialized\")\n",
    "    predictedClasses = neuralNet_.model(num_nodehl1,num_nodehl2,num_nodehl3)\n",
    "    \"\"\"\n",
    "    cross-entropy is a continuous function that is always positive. \n",
    "    If the predicted y equals the true y, cross-entropy equals zero.\n",
    "    Cross-entropy is used in classification.\n",
    "    This measures how well the model works on each image individually. \n",
    "    The softmax_cross_entropy_with_logits function makes the sum of the inputs equal to 1, normalizing the values\n",
    "\n",
    "    \"\"\"\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=predictedClasses, labels=neuralNet_.Y)\n",
    "    cost = tf.reduce_mean( cross_entropy)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learningRate).minimize(cost)\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "    # initialize the weights and biases before optimization starts\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Session started, training begins\")\n",
    "\n",
    "        # train the network\n",
    "        for iter in range(1, epochs+1):\n",
    "            iter_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples / train_batch_size)):\n",
    "                # get the next batch of training examples. \n",
    "                # x_batch holds a batch of images\n",
    "                # y_true_batch holds the true labels for x_batch\n",
    "                x_batch, y_true_batch = mnist.train.next_batch(train_batch_size)\n",
    "\n",
    "                # fd_train holds the batch in a dictionary with the named placeholders\n",
    "                # in the tensorflow graph\n",
    "                fd_train = {neuralNet_.X: x_batch, neuralNet_.Y: y_true_batch}\n",
    "                _, i_loss = sess.run([optimizer, cost], feed_dict=fd_train)\n",
    "\n",
    "                iter_loss += i_loss   \n",
    "            print('iter', iter, 'of', epochs, 'loss: ', iter_loss)\n",
    "            \n",
    "        correct = tf.nn.in_top_k(predictedClasses, neuralNet_.Y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "        print('\\ntraining set accuracy: ', accuracy.eval({neuralNet_.X: mnist.train.images, neuralNet_.Y: mnist.train.labels})) \n",
    "        print('test set accuracy: ', accuracy.eval({neuralNet_.X: mnist.test.images, neuralNet_.Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net initialized\n",
      "Session started, training begins\n",
      "iter 1 of 50 loss:  1586178.25668\n",
      "iter 2 of 50 loss:  81139.4549046\n",
      "iter 3 of 50 loss:  39545.0357552\n",
      "iter 4 of 50 loss:  22513.6690333\n",
      "iter 5 of 50 loss:  13874.724625\n",
      "iter 6 of 50 loss:  8052.13278171\n",
      "iter 7 of 50 loss:  5049.7586467\n",
      "iter 8 of 50 loss:  3174.91844182\n",
      "iter 9 of 50 loss:  1847.91737552\n",
      "iter 10 of 50 loss:  1154.26184219\n",
      "iter 11 of 50 loss:  820.901091038\n",
      "iter 12 of 50 loss:  625.041497518\n",
      "iter 13 of 50 loss:  326.944234612\n",
      "iter 14 of 50 loss:  203.655233412\n",
      "iter 15 of 50 loss:  210.711860754\n",
      "iter 16 of 50 loss:  168.008256282\n",
      "iter 17 of 50 loss:  94.1999906018\n",
      "iter 18 of 50 loss:  61.1429731754\n",
      "iter 19 of 50 loss:  42.6163966988\n",
      "iter 20 of 50 loss:  29.5169159246\n",
      "iter 21 of 50 loss:  23.5825819931\n",
      "iter 22 of 50 loss:  15.5916622055\n",
      "iter 23 of 50 loss:  37.7761814925\n",
      "iter 24 of 50 loss:  21.5687114541\n",
      "iter 25 of 50 loss:  12.2069147498\n",
      "iter 26 of 50 loss:  12.170218416\n",
      "iter 27 of 50 loss:  8.84293152297\n",
      "iter 28 of 50 loss:  2.89698992017\n",
      "iter 29 of 50 loss:  6.73251724336\n",
      "iter 30 of 50 loss:  7.50792042155\n",
      "iter 31 of 50 loss:  7.66337250452\n",
      "iter 32 of 50 loss:  2.23170403607\n",
      "iter 33 of 50 loss:  0.0\n",
      "iter 34 of 50 loss:  0.0\n",
      "iter 35 of 50 loss:  0.0\n",
      "iter 36 of 50 loss:  0.0\n",
      "iter 37 of 50 loss:  0.0\n",
      "iter 38 of 50 loss:  0.0\n",
      "iter 39 of 50 loss:  0.0\n",
      "iter 40 of 50 loss:  0.0\n",
      "iter 41 of 50 loss:  0.0\n",
      "iter 42 of 50 loss:  0.0\n",
      "iter 43 of 50 loss:  0.0\n",
      "iter 44 of 50 loss:  0.0\n",
      "iter 45 of 50 loss:  0.0\n",
      "iter 46 of 50 loss:  0.0\n",
      "iter 47 of 50 loss:  0.0\n",
      "iter 48 of 50 loss:  0.0\n",
      "iter 49 of 50 loss:  0.0\n",
      "iter 50 of 50 loss:  0.0\n",
      "\n",
      "training set accuracy:  1.0\n",
      "test set accuracy:  0.9289\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
